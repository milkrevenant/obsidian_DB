머신러닝 : 라벨(y=종속변수=정답)이 있는가
지도학습 ; 정답지가 있는 데이터, 가지고 있는 정답지를 바탕으로 입력에 대한 출력이 정답지와 같아지게 시스템을 변화시키도록 구성함. === 회귀분석, 분류(knn, svm, 결정나무)
- 결정나무알고리즘은 항복을 배치할 클래스를 결정하는 데 유용한 항목 속성에 대한 질문들을 규명함으로써 항목을 분류하는 것을 목표로 함. 트리 내의 각 노드는 질문이고, 가지는 항목에 대한 더 많은 질문으로 이어지고, 잎은 최종 분류에 해당함.(분류가 목적!!!!!) -- 어떠한 속성을 갖고 있을 때 어떤 결과를 보여주는 지를 분류하는 것이 노드를 통해 정리하는 방법 // 핵심질문을 통해 분류하는 연습을 학습하는 과정이 결정나무 학습과 관련이 있음. 하지만 과접합/오버피팅 의 경향이 있음.(학습할 때 지나치게 분류함, 고양이가 앉아있는 고양이만 있는 것은 아니지만, 앉아 있는 고양이만 고양이로 분류하는 실패를 할 수 있음.)
- 랜덤포레스트 : overfitting을 피하기 위해서 임의의 숲을 구성하는 것 - 다수의 의사결정 트리를 만들어서 분류를 통한 분류를 시도하는 것이 랜덤포레스트


비지도 학습 ; 정답지가 없는 데이터로 학습 데이터 집합에 내재하는 구조를 알아가는 과정. 차원축소(pca, 클러스터링(K-means)

강화 학습 : 구성요소 이해가 필요
agent : 강화학습에서 의사 결정을 수행하는 주체로 환경과 상호작용하며 행동을 결정하고, 그에 따른 보상을 받는다. 에이전트는 현재의 상태를 파아가고 최적의 행동을 선택하기 위한 정책을 갖고 있음.
환경 : 에이전트의 무대로 에이전트의 state와 reward를 결정함.
상태 : state는 환경과 상호작용할 때 어떤 상황에 있는지를 나타내는 것으로 상태는 환경에 따라 다양한 정보를 포함할 수 있음.
행동 : 강화 학습에서는 에이전트가 가능한 행동을 선택함. 행동은 에이전트가 의사 결정을 통해 환경에 대해 행하기로 한 행동.
보상 : 보상은 에이전트가 특정 행동을 취했을 때 받는 신호.
