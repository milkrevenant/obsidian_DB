머신러닝 : 라벨(y=종속변수=정답)이 있는가
지도학습 ; 정답지가 있는 데이터, 가지고 있는 정답지를 바탕으로 입력에 대한 출력이 정답지와 같아지게 시스템을 변화시키도록 구성함. === 회귀분석, 분류(knn, svm, 결정나무)
- 결정나무알고리즘은 항복을 배치할 클래스를 결정하는 데 유용한 항목 속성에 대한 질문들을 규명함으로써 항목을 분류하는 것을 목표로 함. 트리 내의 각 노드는 질문이고, 가지는 항목에 대한 더 많은 질문으로 이어지고, 잎은 최종 분류에 해당함.(분류가 목적!!!!!) -- 어떠한 속성을 갖고 있을 때 어떤 결과를 보여주는 지를 분류하는 것이 노드를 통해 정리하는 방법 // 핵심질문을 통해 분류하는 연습을 학습하는 과정이 결정나무 학습과 관련이 있음. 하지만 과접합/오버피팅 의 경향이 있음.(학습할 때 지나치게 분류함, 고양이가 앉아있는 고양이만 있는 것은 아니지만, 앉아 있는 고양이만 고양이로 분류하는 실패를 할 수 있음.)
- 랜덤포레스트 : overfitting을 피하기 위해서 임의의 숲을 구성하는 것 - 다수의 의사결정 트리를 만들어서 분류를 통한 분류를 시도하는 것이 랜덤포레스트


비지도 학습 ; 정답지가 없는 데이터로 학습 데이터 집합에 내재하는 구조를 알아가는 과정. 차원축소(pca, 클러스터링(K-means)

강화 학습 : 구성요소 이해가 필요
agent : 강화학습에서 의사 결정을 수행하는 주체로 환경과 상호작용하며 행동을 결정하고, 그에 따른 보상을 받는다. 에이전트는 현재의 상태를 파아가고 최적의 행동을 선택하기 위한 정책을 갖고 있음.
환경 : 에이전트의 무대로 에이전트의 state와 reward를 결정함.
상태 : state는 환경과 상호작용할 때 어떤 상황에 있는지를 나타내는 것으로 상태는 환경에 따라 다양한 정보를 포함할 수 있음.
행동 : 강화 학습에서는 에이전트가 가능한 행동을 선택함. 행동은 에이전트가 의사 결정을 통해 환경에 대해 행하기로 한 행동.
보상 : 보상은 에이전트가 특정 행동을 취했을 때 받는 신호.

머신러닝은 가중치를 넘는 것을 기본 원리로 함.
임계점을 넘으면 값 도출, 넘지 않으면 값 도출 안함.

인공신경망 : 하나의 입력과 하나의 출력이 있다면, 입력에 의해 뉴런이 작동함. 작동한다는 것은 꺼져있는 상태(off)에서 켜짐(on)상태로 전환된다는 것인데,
켜짐과 꺼짐은 가중치가 결정함. 값이 임계값(세타)을 넘는다면 1을 출력함. 
가중치 및 편향에 대해서는 추가적으로 chatgpt의 설명을 참고할 필요가 있음.
xorgate는 x와 y 중 하나만 참이어도 참이라는 수식.

단층 퍼셉트론은 가중치와 값이 바로바로 연결되어있는데, 다중 퍼셉트론은 기어가 너무 여러개라서 기어를 조절하기가 쉽지 않음. 다중 퍼셉트론의 등장 이후, 단층 퍼셉트론의 이론적인 문제는 극복하였으나, 실제 안에서 일어나는 학습 방법을 이해하거나 알아내는 것에는 어려움이 있었음.(한계에 부딪쳤던 부분)

미분값이 사라지는 문제는 가중치의 초기값을 조정하며 해결


- 1 에포크는 모든 데이터 셋을 학습하는 횟수를 의미함. 하나의 데이터셋이 신경망을 통과한 것.
(순전파 - 역전파 통과)
- 배치사이즈는 200개의 수학문제를 10개로 작은 그룹으로 나눌 경우 200이 됨.
- 분류는 레이블이 달린 학습 데이터로 학습한 후에, 새로 입력된 데이터가 학습했던 어느 그룹에 속하는 지를 찾아내는 방법.
- 회귀모델은 연속성을 지닌 실수 값을 예측하는 것
- 비지도학습은 레이블되지 않는 데이터를 학습. 초기의 목표값은 없지만, 데이터간의 관련성을 따져서 군집화하는 것이 목표 - 클러스터링, 차원 축소 /// 비지도학습은 그래서 k-means 알고리즘을 활용하여 가까운 데이터끼리 묶는 군집화 방법.(군집의 평균을 구해서 군집에 포함시키는 )
- 차원축소를 이해하기 위해 필요한 개념이 manifold.
- manifold는 고차원의 공간을 저차원으로 매핑하는 subspace를 의미하고,
- 고차원의 공간에 분포된 데이터의 특징을 잘 표현하는 subspace인 manifold를 찾는 것이 차원 축소/감소의 방법이 됨 --- 이를 manifold learning이라고 함.


- 딥러닝의 등장 : 학습은 잘 되었더라도 실제 데이터를 적용시키면 정확도가 떨어지는 상황이 발생, 즉 다중 퍼셉트론의 학습은 훈련 데이터에만 최적화되었고, 실제 데이터에는 효과가 없어서 이를 과적합이라고 불렀다.(훈련에만 최적화된)
- 미분값이 사라지는 문제는 가중치의 초기값을 잘 설정해 해결이 되었고, 과적합문제는 데이터의 특징 추출을 잘하는 비지도학습을 통해 과적합을 방지하는 초기 가중치를 찾고, 역전파 알고리즘으로 미세하게 조정하여 과적합 문제를 해결하게 되고, 이를 딥러닝이라고 부름.

